

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/logo.png">
  <link rel="icon" href="/img/logo.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#fdfdfd">
  <meta name="description" content="">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
  <title>RNN、LSTM、Seq2Seq、Pointer Network - Herlo&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="/css/mycss.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 50vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Herlo's Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                
                Category
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                
                Links
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/bg.svg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="RNN、LSTM、Seq2Seq、Pointer Network">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-10-04 17:44" pubdate>
        2021年10月4日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      2.5k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      29
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">RNN、LSTM、Seq2Seq、Pointer Network</h1>
            
            <div class="markdown-body">
              <p>从基础的RNN网络结构开始，是如何一步步改进得到较为复杂的Pointer Network的。</p>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>与传统的前向神经网络和卷积神经网络 (CNN) 不同，循环神经网络 (Recurrent Neural Networks，RNN)是一种擅于处理<strong>序列数据</strong>的模型，例如文本、时间序列、股票市场等。</p>
<p>对于序列数据，输入之间存在着先后顺序，如“我打车去商场” 和 “我去商场打车”，我们通常需要按照一定的顺序阅读句子才能理解句子的意思。</p>
<p>面对这种情况我们就需要用到循环神经网络了，每一时刻，RNN网络输入输出如下：</p>
<ul>
<li>$x_{t}$ 表示 t 时刻的输入向量(例如第 t 个单词的词向量)。</li>
<li>$h_t$ 表示 t 时刻的隐隐藏层向量 (包含了从开始一直到 t 时刻的相关信息)。</li>
<li>$y_t$ 表示 t 时刻的输出向量 (通常是预测的结果)。</li>
</ul>
<center>    
  <img height="40%" width="40%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化1.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图1 RNN神经元
  </div> 
</center>

<p>在 RNN 中每一时刻都共用同一个神经元，将神经元展开之后如下图所示。</p>
<center>    
  <img height="60%" width="60%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化2.webp?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图2 RNN展开
  </div> 
</center>

<p>在 RNN 中输入$x_{t}$只包含 t 时刻信息，不包含顺序信息；而 $h_t$           是根据$x_{t}$和$h_{t-1}$计算得到的，包含了历史信息与当前输入信息。</p>
<script type="math/tex; mode=display">
h_{t} = \sigma (z_{t}) = \sigma (Ux_t + Wh_{t-1} + b)
 \\
y_t = \sigma (Vh_t + c)</script><p>需要注意的是，对于任意时刻，所有的权值($W$,$U$,$V$)都相等，即“权值共享”，极大的减少参数量。</p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>RNN结构十分简单，但参数矩阵 U 和 W 的梯度均存在长期依赖，当面对一个长序列的时候，由于梯度消失与梯度爆炸，RNN难以发挥作用。1997年，<a target="_blank" rel="noopener" href="http://www.bioinf.jku.at/publications/older/2604.pdf">Hochreiter &amp; Schmidhuber</a>提出一种RNN的变种LSTMs(Long Short Term Memory networks)，尝试解决长期依赖的问题。</p>
<blockquote>
<p>该部分的图出自Christopher Olah的<em>Understanding LSTM Networks</em><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Christopher Olah.Understanding LSTM Networks
">[1]</span></a></sup></p>
</blockquote>
<p>在传统RNN中，循环单元结构简单，只有一层网络层，如单层tanh层。</p>
<center>    
  <img height="100%" width="100%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化3.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图3 拥有一层tanh层的RNN神经元
  </div> 
</center>

<p>为了应对长期依赖，LSTM的循环单元结构更为复杂，他不再只有一层，取而代之的是多层神经网络层，他们各有不同的功能，且以特殊的方式互相连接在一起。</p>
<center>    
  <img height="100%" width="100%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化4.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图4 LSTMs的循环单元有更复杂的结构
  </div> 
</center>

<p>上图中的一些符号含义如下</p>
<center>    
  <img height="80%" width="80%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化5.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图5 一些符号
  </div> 
</center>

<p>带箭头的线代表向量的流动方向，分叉代表向量流向多个节点，合并代表向量以某种方式共同作用；黄色矩形代表神经网络层，拥有相应的权重、偏置、激活函数；粉色圆形代表一些向量运算，如按位乘等。</p>
<p>LSTMs的一个关键设计在于$C_t$的引入，他经过“遗忘门”、“输入门”的作用，最终在“输入门”影响最终的输入$h$。</p>
<center>    
  <img height="50%" width="50%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化6.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图6 cell state
  </div> 
</center>

<p>另一关键设计是”门”的引入，他由sigmoid层与按位乘运算组成。LSTMs有三个门：<strong>遗忘门</strong>、<strong>输入门</strong>、<strong>输出门</strong>，我们来逐步了解他们是如何起作用的。</p>
<center>    
  <img height="20%" width="20%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化7.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图7 LSTMs中的“门”
  </div> 
</center>

<p>首先是遗忘门，用来判断哪些信息应该删除。其中$\sigma$表示激活函数 sigmoid。输入的$h_{t-1}$和$x_t$经过 sigmoid 激活函数之后得到$f_t$，$f_t$中每一个值的范围都是 [0, 1]。$f_t$中的值越接近 1，表示对应位置的值更应该记住；越接近 0，表示对应位置的值更应该忘记。将 $f_t$与$C_{t-1}$按位相乘 (ElementWise 相乘)，即可以得到遗忘无用信息之后的$C_{t-1}^{‘}$。</p>
<center>    
  <img height="100%" width="100%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化8.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图8 遗忘门
  </div> 
</center>



<p>其次是输入门，用来判断哪些新的信息应该记住。输入的$h_{t-1}$和$x_t$经过 tanh 激活函数可以得到新的输入信息$\tilde{C_{t}}$，但是这些新信息并不全是有用的，因此需要使用$h_{t-1}$和$x_t$经过sigmoid 函数得到$i_t$，$i_t$表示哪些新信息是有用的。两向量相乘后的结果加到$C_{t-1}^{‘}$中，即得到$C_t$。</p>
<center>    
  <img height="100%" width="100%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化9.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图9 输入门
  </div> 
</center>

<p>现在，我们已经知道如何更新$C$了</p>
<center>    
  <img height="100%" width="100%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化10.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图10 更新cell state
  </div> 
</center>

<p>最后是输出门，用来判断应该输出哪些信息到$h_t$中。$C_t$经过 tanh 函数(范围变为[-1, 1])得到应该输出的信息，然后$h_{t-1}$和$x_t$经过 sigmoid 函数得到一个向量$o_t$(范围[0, 1])，表示哪些位置的输出应该去掉，哪些应该保留。两向量相乘后的结果就是最终的$h_t$。</p>
<center>    
  <img height="100%" width="100%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化11.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图11 输出门
  </div> 
</center>

<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><p>Seq2Seq在2014年首次由Sutskever等人在<em>Sequence to Sequence Learning with Neural Networks</em><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Sutskever I, Vinyals O, Le Q V. Sequence to Sequence Learning with Neural Networks[C]//Advances in neural information processing systems. 2014: 3104-3112.
">[2]</span></a></sup>一文中提出，同年份Yoshua Bengio团队的<em>Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translatio</em>n<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation[J]. arXiv preprint arXiv:1406.1078, 2014.
">[3]</span></a></sup>也独立的阐述了Seq2Seq的主要思想。</p>
<blockquote>
<p>两篇文章中提出的网络模型有些不同，这里主要围绕Sutskever等人提出的模型展开，借鉴于<em>Natural Language Processing with Deep Learning</em><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Guillaume Genthial, Lucas Liu, Barak Oshri, Kushal Ranjan. CS224n: Natural Language Processing with Deep Learning , 2017.
">[4]</span></a></sup>。</p>
</blockquote>
<p>Seq2Seq模型是一个端到端模型，可以看作由两个RNN网络(通常为LSTM)组成</p>
<ul>
<li>编码器，根据输入产生固定大小的中间语义向量$C$</li>
<li>解码器，根据中间语义向量$C$产生最终输出</li>
</ul>
<p>接下来分别介绍编码器与解码器是如何工作的</p>
<center>    
  <img height="30%" width="30%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化12.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图12 编码器
  </div> 
</center>

<p>编码器需要接收输入序列，产生中间语义向量，但是将一个任意长度的输入序列转换为固定长度的$C$对于单层架构来说太困难了，通常我们会使用多层，图中展示的是三层LSTMs架构，最后一层LSTM层的输出将作为$C$。</p>
<p>此外，Seq2Seq通常会逆序输入原序列(注意图中Timesteps的箭头指向)，这样一来，编码器最后的输出将会是原序列的首个元素，解码器在解码过程中遇到的首个输入正好对应原序列的首元素。这对最终结果是有利的，如文本任务，当解码器正确翻译了前面一部分单词，将很容易猜出完整的句子。</p>
<center>    
  <img height="30%" width="30%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化13.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图13 解码器
  </div> 
</center>

<p>解码器同样是一个LSTM多层网络，但结构更复杂一些，每一层的输出将作为下一层的输入。使用上文中得到的$C$初始化解码器，并输入一个开始信号(在文本任务中，通常为<code>&lt;EOS&gt;</code>)，最终得到输出序列，输入序列和输出序列长度不要求相同。</p>
<p>得到输出序列后，我们可以定义loss，使用梯度下降和后向传播最小化loss，训练我们的编码器和解码器。</p>
<p class="note note-primary">
  <b>Bidirectional RNNs</b>
</p>

<p>对于某个句子，当前的单词也许和它之前、之后两个方向的单词均有联系，但是目前的Seq2Seq结构只考虑了一个方向。Bidirectional RNNs以一种巧妙的结构解决了该问题。</p>
<center>    
  <img height="30%" width="30%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化14.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图14 单层bi-LSTM编码器示例
  </div> 
</center>

<p>Bidirectional RNNs以前向、后向两种方式输入序列，输出也是由前向RNN和后向RNN共同组成的。</p>
<p class="note note-primary">
  <b>Attention机制</b>
</p>

<p>一个句子中不同的单词，给予的关注度是不同的，如“the ball is on the field”，你会更关注”ball,” “on,” 和”field,”。因此， Bahdanau等人提出了一种解决方案<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Bahdanau D, Cho K, Bengio Y. Bahdanau et al., 2014. Neural Machine Translation by Jointly Learning to Align and Translate[J]. arXiv preprint arXiv:1409.0473, 2014.
">[5]</span></a></sup>，其中设计的关键一点便是Attention机制。</p>
<center>    
  <img height="30%" width="30%" src="https://github.com/HerloConnell/BlogPicture/blob/Optimization-problem/基于深度学习的组合优化15.png?raw=true" srcset="/img/loading.gif" lazyload>    
  <br>    
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">
    图15 由给定的输入序列x生成t时刻的输出y
  </div> 
</center>

<p>首先是编码器，使用bi-LSTM结构，设编码器隐藏层输出为$(h_1,…,h_n)$。</p>
<p>其次是解码器，这里与之前的大有不同，设$s_i$为$i$时刻隐藏层输出(隐状态)，其计算方法如下：</p>
<script type="math/tex; mode=display">
s_i = f(s_{i-1}, y_{i-1}, c_i)</script><p>值得注意的是，不同于之前只有一个语义向量$C$的Seq2Seq，这里每个$y_i$均对应一个$c_i$，其中$c_i$是编码器隐状态的加权和。</p>
<script type="math/tex; mode=display">
c_i= \sum_{j=1}^{T_x}{\alpha_{i,j}h_j}</script><p>$T_x$代表编码器隐状态总数，即输入序列长度。</p>
<p>权重$\alpha_{i,j}$由$e_{i,j}$经过$softmax$运算计算而来</p>
<script type="math/tex; mode=display">
\alpha _{i,j} = \frac{exp(e_{i,j})}{\sum_{k=1}^{T_x}exp(e_{i,k}) }</script><p>那么$e_{i,j}$又是如何计算的呢</p>
<script type="math/tex; mode=display">
e_{i,j} = a(s_{i-1}, h_j)</script><p>其中$a$代表某种对齐模型（如单层全连接层），即对$j$位置的输入和$i$时刻的输出，二者的匹配程度做一个评估，得到评估分数$e_{i,j}$。</p>
<p>可以看出，Attention的作用就是将encoder的隐状态按照一定权重加和之后拼接到decoder的隐状态上，以此作为额外信息，起到“软对齐”的作用，并且提高了整个模型的预测准确度。简单举个例子，在机器翻译中一直存在对齐的问题，也就是说源语言的某个单词应该和目标语言的哪个单词对应，如“Who are you”对应“你是谁”，如果我们简单地按照顺序进行匹配的话会发现单词的语义并不对应，显然“who”不能被翻译为“你”。而Attention机制非常好地解决了这个问题。如前所述，Attention会给输入序列的每一个元素分配一个权重，如在预测“你”这个字的时候输入序列中的“you”这个词的权重最大，这样模型就知道“你”是和“you”对应的，从而实现了软对齐。</p>
<h1 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h1><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>此外，感谢以下博客，做了很好的总结</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/jerr__y/article/details/53749693">永永夜. seq2seq学习笔记</a></p>
<section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Christopher Olah.<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>
<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Sutskever I, Vinyals O, Le Q V. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a>[C]//Advances in neural information processing systems. 2014: 3104-3112.
<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Cho K, Van Merriënboer B, Gulcehre C, et al. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.1078">Learning phrase representations using RNN encoder-decoder for statistical machine translation</a>[J]. arXiv preprint arXiv:1406.1078, 2014.
<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Guillaume Genthial, Lucas Liu, Barak Oshri, Kushal Ranjan. <a target="_blank" rel="noopener" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lecture_notes/cs224n-2017-notes6.pdf">CS224n: Natural Language Processing with Deep Learning </a>, 2017.
<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Bahdanau D, Cho K, Bengio Y. Bahdanau et al., 2014. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>[J]. arXiv preprint arXiv:1409.0473, 2014.
<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                  </div>
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/09/05/%E8%8B%B1%E8%AF%AD%E8%AF%AD%E6%B3%95/">
                        <span class="hidden-mobile">英语语法</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js" ></script>

  





  <script  src="https://cdn.jsdelivr.net/npm/mermaid@8.10.1/dist/mermaid.min.js" ></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({"theme":"default"});
    }
  </script>







<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
