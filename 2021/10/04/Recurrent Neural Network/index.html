

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/logo.png">
  <link rel="icon" href="/img/logo.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#fdfdfd">
  <meta name="description" content="">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
  <title>图解RNN、LSTM、Seq2Seq、Attention、Pointer Network - Herlo&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="/css/mycss.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 50vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Herlo's Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                
                Category
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                
                Links
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/bg.svg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="图解RNN、LSTM、Seq2Seq、Attention、Pointer Network">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-10-04 17:44" pubdate>
        2021年10月4日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      2.9k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      34
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">图解RNN、LSTM、Seq2Seq、Attention、Pointer Network</h1>
            
            <div class="markdown-body">
              <p>从基础的RNN网络结构开始，介绍LSTM、Seq2Seq、Attention、Pointer Network。</p>
<h1 id="RNNs"><a href="#RNNs" class="headerlink" title="RNNs"></a>RNNs</h1><p>与传统的前向神经网络和卷积神经网络不同，循环神经网络 (Recurrent Neural Networks，RNNs) 是一种擅于处理<strong>序列数据</strong>的模型，例如文本、时间序列、股票市场等。</p>
<p>对于序列数据，输入之间存在着先后顺序，如“我打车去商场” 和 “我去商场打车”，我们通常需要按照一定的顺序阅读句子才能理解句子的意思。</p>
<blockquote>
<p>该部分图出自Afshine Amidi 和 Shervine Amidi 的 Recurrent Neural Networks cheatsheet<sup id="fnref:0" class="footnote-ref"><a href="#fn:0" rel="footnote"><span class="hint--top hint--rounded" aria-label=" Afshine Amidi,Shervine Amidi. Recurrent Neural Networks cheatsheet
">[0]</span></a></sup></p>
</blockquote>
<p>面对这种情况我们就需要用到循环神经网络了，其结构如下：</p>
<p><img src="https://i.loli.net/2021/10/16/4pJsfiPgcvZDyNB.png" srcset="/img/loading.gif" lazyload alt="RNN结构"></p>
<p>$x^{T}$为我们的输入序列，$a^{T}$为隐藏层输出，它保存了输入序列的历史信息，$y^{T}$为输出序列，计算方法如下</p>
<script type="math/tex; mode=display">
\begin{align}
a^{<t>} = g_{1}(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_{a})
 \\
y^{<t>} = g_{2} (W_{ya}a^{<t>} + b_{y})
\end{align}</script><p>其中，$W_{ax}$，$W_{aa}$，$W_{ya}$，$b_a$，$b_y$，$W_{ax}$，$W_{aa}$，$W_{ya}$，$b_a$，$b_y$ 为模型参数，由所有时刻$t$共享， $g_1$，$g_2$ 为激活函数。</p>
<p><img src="https://i.loli.net/2021/10/16/mYIvgnV59NKoPXB.png" srcset="/img/loading.gif" lazyload alt="单个RNN单元内部结构"></p>
<p>$\mathcal{L}$  定义为各时刻$\mathcal{L}$ 之和，每一时刻$t$都将进行loss计算，计算偏导数，更新网络参数</p>
<script type="math/tex; mode=display">
\mathcal{L}(\hat{y}, y) = \sum^{T_y}_{t = 1}\mathcal{L}(\hat{y}^{<t>}, y^{<t>})</script><p>这样一个基础的RNN结构，隐藏层的存在让他拥有“记忆”的能力，当然它也存在一些不足。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>😆</th>
<th>😭</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入序列长度可变，模型size不会随着输入序列长度增加而增加</td>
<td>计算缓慢</td>
</tr>
<tr>
<td>“记忆”历史信息</td>
<td>长期记忆能力不足</td>
</tr>
<tr>
<td>权重在任意时刻共享</td>
<td>看不到未来的信息</td>
</tr>
</tbody>
</table>
</div>
<p>面对不同的应用场景，RNN拥有多种结构：</p>
<ul>
<li><strong>输入不是序列而输出为序列，如根据类别产生音乐</strong></li>
</ul>
<p><img src="https://i.loli.net/2021/10/16/w3YJghbM74sezoZ.png" srcset="/img/loading.gif" lazyload alt="$T_x$ = 1, $T_y$ > 1" style="zoom:50%;" /></p>
<ul>
<li><strong>输入是序列而输出不是序列，如情感分析</strong></li>
</ul>
<p><img src="https://i.loli.net/2021/10/16/nZFBRSwj9J37LeD.png" srcset="/img/loading.gif" lazyload alt="$T_x > 1$, $T_y = 1$" style="zoom:50%;" /></p>
<ul>
<li><strong>输入是序列而输出也是序列，且等长，传统RNN结构</strong></li>
</ul>
<p><img src="https://i.loli.net/2021/10/16/wF75QY4ckRxPLnW.png" srcset="/img/loading.gif" lazyload alt="$T_x$ = $T_y$" style="zoom:50%;" /></p>
<ul>
<li><strong>输入是序列而输出也是序列，且不等长，如下文我们提到的encoder-decoder模型</strong></li>
</ul>
<p><img src="https://i.loli.net/2021/10/16/EAG3NZawTtbgQm2.png" srcset="/img/loading.gif" lazyload alt="$T_x$ != $T_y$" style="zoom:50%;" /></p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>RNN结构十分简单，但参数矩阵 U 和 W 的梯度均存在长期依赖，当面对一个长序列的时候，由于梯度消失/爆炸，RNN难以发挥作用。1997年，Hochreiter &amp; Schmidhuber提出一种RNN的变种LSTMs(Long Short Term Memory networks)，尝试解决长期依赖的问题。</p>
<blockquote>
<p>该部分的图出自Christopher Olah的<em>Understanding LSTM Networks</em><sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Christopher Olah.Understanding LSTM Networks
">[1]</span></a></sup></p>
</blockquote>
<p>在传统RNN中，循环单元结构简单，只有一层网络层，如单层tanh层。</p>
<p><img src="https://i.loli.net/2021/10/16/liqRmNCbaxvYhdW.png" srcset="/img/loading.gif" lazyload alt="拥有一层tanh层的RNN神经元" style="zoom:100%;" /></p>
<p>为了应对长期依赖，LSTM的循环单元结构更为复杂，他不再只有一层，取而代之的是多层神经网络层，他们各有不同的功能，且以特殊的方式互相连接在一起。</p>
<p><img src="https://i.loli.net/2021/10/16/liqRmNCbaxvYhdW.png" srcset="/img/loading.gif" lazyload alt="LSTMs的循环单元有更复杂的结构" style="zoom:100%;" /></p>
<p>上图中的一些符号含义如下</p>
<p><img src="https://i.loli.net/2021/10/16/Xp9FeUaErZQDWgA.png" srcset="/img/loading.gif" lazyload alt="一些符号" style="zoom:100%;" /></p>
<p>带箭头的线代表向量的流动方向，分叉代表向量流向多个节点，合并代表向量以某种方式共同作用；黄色矩形代表神经网络层，拥有相应的权重、偏置、激活函数；粉色圆形代表一些向量运算，如按位乘等。</p>
<p>LSTMs的一个关键设计在于$C_t$的引入，他经过“遗忘门”、“输入门”的作用，最终在“输入门”影响最终的输入$h$。</p>
<p><img src="https://i.loli.net/2021/10/16/JTyZBOQGgceUxjI.png" srcset="/img/loading.gif" lazyload alt="cell state" style="zoom:100%;" /></p>
<p>另一关键设计是”门”的引入，他由sigmoid层与按位乘运算组成。LSTMs有三个门：<strong>遗忘门</strong>、<strong>输入门</strong>、<strong>输出门</strong>，我们来逐步了解他们是如何起作用的。</p>
<p><img src="https://i.loli.net/2021/10/16/wu9fQZWOxKSHGa4.png" srcset="/img/loading.gif" lazyload alt="LSTMs中的“门”" style="zoom:80%;" /></p>
<p>首先是遗忘门，用来判断哪些信息应该删除。其中$\sigma$表示激活函数 sigmoid。输入的$h_{t-1}$和$x_t$经过 sigmoid 激活函数之后得到$f_t$，$f_t$中每一个值的范围都是 [0, 1]。$f_t$中的值越接近 1，表示对应位置的值更应该记住；越接近 0，表示对应位置的值更应该忘记。将 $f_t$与$C_{t-1}$按位相乘 (ElementWise 相乘) ，即可以得到遗忘无用信息之后的$C_{t-1}^{‘}$。</p>
<p><img src="https://i.loli.net/2021/10/16/3ixgGztSNAcey4o.png" srcset="/img/loading.gif" lazyload alt="遗忘门" style="zoom:100%;" /></p>
<p>其次是输入门，用来判断哪些新的信息应该记住。输入的$h_{t-1}$和$x_t$经过 tanh 激活函数可以得到新的输入信息$\tilde{C_{t}}$，但是这些新信息并不全是有用的，因此需要使用$h_{t-1}$和$x_t$经过sigmoid 函数得到$i_t$，$i_t$表示哪些新信息是有用的。两向量相乘后的结果加到$C_{t-1}^{‘}$中，即得到$C_t$。</p>
<p><img src="https://i.loli.net/2021/10/16/BHkoO9ZEedpXWnF.png" srcset="/img/loading.gif" lazyload alt="输入门" style="zoom:100%;" /></p>
<p>现在，我们已经知道如何更新$C$了</p>
<p><img src="https://i.loli.net/2021/10/16/9wyXEuCjDq6bHPr.png" srcset="/img/loading.gif" lazyload alt="更新cell state" style="zoom:100%;" /></p>
<p>最后是输出门，用来判断应该输出哪些信息到$h_t$中。$C_t$经过 tanh 函数(范围变为[-1, 1])得到应该输出的信息，然后$h_{t-1}$和$x_t$经过 sigmoid 函数得到一个向量$o_t$ (范围[0, 1]) ，表示哪些位置的输出应该去掉，哪些应该保留。两向量相乘后的结果就是最终的$h_t$。</p>
<p><img src="https://i.loli.net/2021/10/16/NVC6uIvJf1Fh7yi.png" srcset="/img/loading.gif" lazyload alt="输出门" style="zoom:100%;" /></p>
<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><p>Seq2Seq (Sequence-to-Sequence) 模型在2014年首次由Sutskever等人在<em>Sequence to Sequence Learning with Neural Networks</em><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Sutskever I, Vinyals O, Le Q V. Sequence to Sequence Learning with Neural Networks[C]//Advances in neural information processing systems. 2014: 3104-3112.
">[2]</span></a></sup>一文中提出，同年份Yoshua Bengio团队的<em>Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translatio</em>n<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation[J]. arXiv preprint arXiv:1406.1078, 2014.
">[3]</span></a></sup>也独立的阐述了Seq2Seq的主要思想。</p>
<blockquote>
<p>两篇文章中提出的网络模型有些不同，这里主要围绕Sutskever等人提出的模型展开，借鉴于<em>Natural Language Processing with Deep Learning</em><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Guillaume Genthial, Lucas Liu, Barak Oshri, Kushal Ranjan. CS224n: Natural Language Processing with Deep Learning , 2017.
">[4]</span></a></sup>。</p>
</blockquote>
<p>Seq2Seq是一个端到端模型，可以看作由两个RNN网络(通常为LSTM)组成</p>
<ul>
<li>编码器，根据输入产生固定大小的中间语义向量$C$。</li>
<li>解码器，根据中间语义向量$C$产生最终输出。</li>
</ul>
<p>接下来分别介绍编码器与解码器是如何工作的</p>
<p><img src="https://i.loli.net/2021/10/16/xGOZuyrFn37jMUq.png" srcset="/img/loading.gif" lazyload alt="编码器" style="zoom:70%;" /></p>
<p>编码器需要接收输入序列，产生中间语义向量，但是将一个任意长度的输入序列转换为固定长度的$C$对于单层架构来说太困难了，通常我们会使用多层，图中展示的是三层LSTMs架构，最后一层LSTM层的输出作为$C$。</p>
<p>此外，Seq2Seq通常会逆序输入原序列 (注意图中Timesteps的箭头指向) ，这样一来，编码器最后的输出将会是原序列的首个元素，解码器在解码过程中遇到的首个输入正好对应原序列的首元素。这对最终结果是有利的，如文本任务，当解码器正确翻译了前面一部分单词，将很容易猜出完整的句子。</p>
<p><img src="https://i.loli.net/2021/10/16/YQFLm86izodvkht.png" srcset="/img/loading.gif" lazyload alt="解码器" style="zoom:70%;" /></p>
<p>解码器同样是一个LSTM多层网络，但结构更复杂一些，每一层的输出将作为下一层的输入。使用上文中得到的$C$初始化解码器，并输入一个开始信号(在文本任务中，通常为<code>&lt;EOS&gt;</code>)，最终得到输出序列，输入序列和输出序列长度不要求相同。</p>
<p>得到输出序列后，我们可以定义loss，使用梯度下降和后向传播最小化loss，训练我们的编码器和解码器。</p>
<p class="note note-primary">
  <b>Bidirectional RNNs</b>
</p>

<p>对于某个句子，当前的单词也许和它之前、之后两个方向的单词均有联系，目前的Seq2Seq结构只考虑了一个方向。Bidirectional RNNs以一种巧妙的结构解决了该问题。</p>
<p><img src="https://i.loli.net/2021/10/16/3ztWiMYHQhwZBm7.png" srcset="/img/loading.gif" lazyload alt="单层bi-LSTM编码器示例" style="zoom:70%;" /></p>
<p>Bidirectional RNNs以前向、后向两种方式输入序列，输出也是由前向RNN和后向RNN共同组成的。</p>
<p class="note note-primary">
  <b>Attention机制</b>
</p>

<p>一个句子中不同的单词，给予的关注度是不同的，如“the ball is on the field”，你会更关注”ball,” “on,” 和”field,”。因此， Bahdanau等人提出了一种解决方案<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Bahdanau D, Cho K, Bengio Y. Bahdanau et al., 2014. Neural Machine Translation by Jointly Learning to Align and Translate[J]. arXiv preprint arXiv:1409.0473, 2014.
">[5]</span></a></sup>，其中设计的关键一点便是Attention机制。</p>
<p><img src="https://i.loli.net/2021/10/16/OerqiZwXcBT4pan.png" srcset="/img/loading.gif" lazyload alt="由给定的输入序列x生成t时刻的输出y" style="zoom:70%;" /></p>
<p>首先是编码器，使用bi-LSTM结构，设编码器隐藏层输出为$(h_1,…,h_n)$。</p>
<p>其次是解码器，这里与之前的大有不同，设$s_i$为$i$时刻隐藏层输出 (隐状态) ，其计算方法如下：</p>
<script type="math/tex; mode=display">
s_i = f(s_{i-1}, y_{i-1}, c_i)</script><p>值得注意的是，不同于之前只有一个语义向量$C$的Seq2Seq，这里每个$y_i$均对应一个$c_i$，其中$c_i$是编码器隐状态的加权和。</p>
<script type="math/tex; mode=display">
c_i= \sum_{j=1}^{T_x}{\alpha_{i,j}h_j}</script><p>$T_x$代表编码器隐状态总数，即输入序列长度。权重$\alpha_{i,j}$由$e_{i,j}$经过$softmax$运算计算而来</p>
<script type="math/tex; mode=display">
\alpha _{i,j} = \frac{exp(e_{i,j})}{\sum_{k=1}^{T_x}exp(e_{i,k}) }</script><p>那么$e_{i,j}$又是如何计算的呢</p>
<script type="math/tex; mode=display">
e_{i,j} = a(s_{i-1}, h_j)</script><p>其中$a$代表某种对齐模型 (如单层全连接层) ，即对于$j$位置的输入和$i$时刻的输出，二者的匹配程度做一个评估，得到的评估分数就是$e_{i,j}$。</p>
<p>可以看出，Attention的作用就是将encoder的隐状态按照一定权重加和之后拼接到decoder的隐状态上，以此作为额外信息，起到“软对齐”的作用，并且提高了整个模型的预测准确度。</p>
<p>简单举个例子，在机器翻译中一直存在对齐的问题，也就是说源语言的某个单词应该和目标语言的哪个单词对应，如“Who are you”对应“你是谁”，如果我们简单地按照顺序进行匹配的话会发现单词的语义并不对应，显然“who”不能被翻译为“你”。</p>
<p>而Attention机制非常好地解决了这个问题，如前所述，Attention会给输入序列的每一个元素分配一个权重，如在预测“你”这个字的时候输入序列中的“you”这个词的权重最大，这样模型就知道“你”是和“you”对应的，从而实现了软对齐。</p>
<h1 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h1><p>为了解决输出序列长度不固定，随输入序列变化而变化的问题 (如组合优化问题) ，2015年，Vinyals 等人对Seq2Seq中的Attention机制进行了改进，提出了指针网络模型 (Pointer Network, Ptr-Net)<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Vinyals O, Fortunato M, Jaitly N. Pointer networks[J]. arXiv preprint arXiv:1506.03134, 2015.
">[6]</span></a></sup>，并展示了如何使用Ptr-Net求得三种几何问题 (凸包问题，计算Delaunay三角形，TSP问题) 的近似解，文章中还表明，Ptr-Net面对测试集的表现并不局限训练模型，拥有良好的泛化能力。</p>
<p><img src="https://i.loli.net/2021/10/16/uHp6LMWsiVP15K4.png" srcset="/img/loading.gif" lazyload alt="以凸包问题为例，Seq2Seq与Ptr-Net的区别" style="zoom:100%;" /></p>
<p>上文已提到，Ptr-Net的主要改进在于Attention机制。设输入序列$P = (P_1,…,P_n)$，encoder和decoder的隐状态分别为$(e_1,…,e_n)$，$(d_1,…,d_n)$。</p>
<script type="math/tex; mode=display">
\begin{align}
u_j^i = v^Ttanh(W_1e_j + W_2d_i), j \in (1,...,n)
 \\
 a_j^i = softmax(u_j^i), j \in (1,...,n)
 \\
 d'_i = \sum^n_{j=1}a_j^ie_j
\end{align}</script><p>$a_j^i$正是针对输入序列的权重，完全可以把它拿出来作为指向输入序列的指针，在每次预测一个元素的时候找到输入序列中权重最大的那个元素即可。Vinyals 等人按照该思路对传统Attention机制进行了修改和简化，公式变成：</p>
<script type="math/tex; mode=display">
\begin{align}
u_j^i = v^Ttanh(W_1e_j + W_2d_i), j \in (1,...,n)
 \\
p(C_i|C_1,...,C_{i-1},P) = softmax(u^i)
\end{align}</script><p>可以看出，Ptr-Net直接将softmax之后得到的$a$作为输出，承担指向输入序列特定元素的指针角色。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:0" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://twitter.com/afshinea">Afshine Amidi</a>,<a target="_blank" rel="noopener" href="https://twitter.com/shervinea">Shervine Amidi</a>. Recurrent Neural Networks cheatsheet
<a href="#fnref:0" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:1" class="footnote-text"><span>Christopher Olah.<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>
<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Sutskever I, Vinyals O, Le Q V. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a>[C]//Advances in neural information processing systems. 2014: 3104-3112.
<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Cho K, Van Merriënboer B, Gulcehre C, et al. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.1078">Learning phrase representations using RNN encoder-decoder for statistical machine translation</a>[J]. arXiv preprint arXiv:1406.1078, 2014.
<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Guillaume Genthial, Lucas Liu, Barak Oshri, Kushal Ranjan. <a target="_blank" rel="noopener" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lecture_notes/cs224n-2017-notes6.pdf">CS224n: Natural Language Processing with Deep Learning </a>, 2017.
<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Bahdanau D, Cho K, Bengio Y. Bahdanau et al., 2014. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>[J]. arXiv preprint arXiv:1409.0473, 2014.
<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Vinyals O, Fortunato M, Jaitly N. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.03134.pdf">Pointer networks</a>[J]. arXiv preprint arXiv:1506.03134, 2015.
<a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                  </div>
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/10/15/%E5%91%A8%E6%8A%A5/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">周报</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/09/05/%E8%8B%B1%E8%AF%AD%E8%AF%AD%E6%B3%95/">
                        <span class="hidden-mobile">英语语法</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="twikoo"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/twikoo@1.3.1/dist/twikoo.all.min.js', function() {
        var options = Object.assign(
          {"envId":"blogs-8gjs26956a49a40f","region":"ap-guangzhou","path":null},
          {
            el: '#twikoo',
            path: ''
          }
        )
        twikoo.init(options)
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js" ></script>

  





  <script  src="https://cdn.jsdelivr.net/npm/mermaid@8.10.1/dist/mermaid.min.js" ></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({"theme":"default"});
    }
  </script>







<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
