---
layout: post
title: Schedule
date: 2021-10-15 11:29
comments: true
external-url:
categories: Others
---

> my daily lift.

## 10.9～10.15


一开始尝试使用pytorch复现ptr-net，但是发现框架基础不足，该计划不太现实。随后，阅读了pytorch官方Tutorials及Docs，并做了一些小的练习。接下来的计划是：

1. 完成上周RNN的笔记
2. 再次尝试复现ptr-net

## 10.16～10.25


花费了几天的时间看[github ptr-net pytorch](https://github.com/shirgur/PointerNet)，难以推进下去，主要原因是attention的细节了解不足。在22号时找到[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)之后终于豁然开朗，到25号下午为止刚完成模型代码的debug，暂未进行实验。

在复现过程中，遇到的值得注意的一点就是：pytorch在反向传播的过程中，不允许需要梯度计算的参数进行就地操作inplace operation，否则会报错：

```text
one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [4, 6]], which is output 0 of SoftmaxBackward, is at version 1; expected version 0 instead. 
Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
```

常见的会犯错误的就地操作有

- `x += 1 `
- `alpha[visted] = 0`

由于是25号下午刚发现的bug，对于`就地操作`还有待进一步总结。

接下来的计划是，首先继续进行ptrnet的实验，查看模型复现效果，然后根据模型复现及实验中遇到的细节问题完善笔记。

## 10.26～10.31


这周进度有点慢了，总结如下：

10.26~10.27，由于理发师的一些过失，效率很低，以后应该及时调整心态；

10.28～10.29，尝试跑了代码，平均loss较高，在1左右，之后询问了师兄师姐如何调整参数，看了"ATTENTION, LEARN TO SOLVE ROUTING PROBLEMS!"的一点开头，写了一部分blog，没有写完；

10.30~10.31，参与了CCPC；

今晚之前把blog赶完吧。

## 11.1～？

11.1，

